---
layout: post
title: from Linguistics to Neural Networks 
excerpt: "A brief summary of NLP."
categories: [CS]
comments: true
image:
  feature: NLP_cover.jpg
---

**Refer to:**

 - Neural Network Methods for Natural Language Processing \\(E_L\\)
 - CS224n
 - d2l

---
# theores

---
 
## Pipeline

- raw text
- extract linguistic features
- retrieve feature vectors, combine them into input vectors
- feed the input into a neural network

## Features

### Features (Linguistics)

#### Features for single words

 - Lemmas: the dictionary entry. *eg. booking, booked, books -> book*.
 - Stems: coarser process than lemmatization. *eg. picture, pictures -> pictur*.
 - Lexical resource: contain information about words, linking them to other words and/or providing additional information. Famous datasets like: WordNet, FrameNet, VerbNet...

#### Features for Text

 - Bag of words: considering each word count as a feature.
 - Weighting: integrate statistics based on external information, eg. word frequency (TF-IDF, ngrams).

#### Features of Words in Context

 - Window: other words left or right to the word.
 - Position: absolute position within a sentence.

#### Features for Word Relations

look at the distance between the words and the identities of the words that appear between them...
   
### Core Features vs. Combination Features

hand-crafted feature-set vs. neural network models (pick up on the important combinations on its own)

### NGram Features

**ngrams**: consecutive word sequences of a given length.
bag-of-bigrams representation is much more powerful than bag-of-words.
multi-layer perceptrons can infer ngrams when applied to a fixed size windows with positional information.

### Distributional Features

The meaning of a word can be inferred from the contexts in which it is used.
Embedding words into vectors to represent the similarity between words.

## Input

Convert text features to feature vectors that can serve as inputs to a model.

### Encodings

#### One-Hot Encodings

assign a unique dimension for each possible feature.
sparse vector.
features are completely independent from one another! ('localist' representation) High-dimensional sparse vectors also cost a lot computational powers

#### Dense Encodings (Feature Embeddings)

each core feature is embedded into a d dimensional space, and represented as a vector in that space.
the dimension d is usually much smaller than the number of features.
the feature-to-vector mappings come from an embedding table (pretrained).

### Combining Dense Vectors

concatenation (to keep position information), summation, weighted summation, affine transformation, averaging, ...

PS: affine transformation of vector \\(mathbf{x}, \mathbf{y}, \mathbf{z}$ -> \\(\mathbf{x}U+\mathbf{y}V+\mathbf{z}W+b\\)

### Variable Number of Features (Unfixed Dimensional Input)

*continuous bag of words(CBOW)*
\\(CBOW(f_1, \cdots, f_k)=\frac{1}{k}\sum_{i=1}^kv(f_i)\\)
(\\(f_i\\) is the ith feature, \\(v(f_i)\\) is its corresponding feature vector)
similar to bag of words, except that \\(v(f_i)\\) in BOW is one-hot encodings.

*weighted continuous bag of words(WCBOW)*
\\(CBOW(f_1, \cdots, f_k)=\frac{1}{\sum_{i=1}^ka_i}\sum_{i=1}^ka_iv(f_i)\\)
$a_i$ is the associated weight of feature $f_i$.

### Paddings, Unknown Words

 - Paddings: add PADDING symbols if the sequence is not long enough.
 - Unknown Words: replace words not in the vocabulary with a UNKNOWN symbol. or replace them with word signatures, eg. replace words that end with ing with a *__ing* symbol.
 - Word Dropout: drop some words (replace them with UNKNOWN symbols) by some strategy, eg. dropout less frequent words with a higher probability. Word dropout can be also used as regularization.

## Language Models

the task of **language modeling** is to assign a probability to any sequences of words $w_{1:n}$.
$$P(w_{1:n})=P(w_1)P(w_2|w_1)P(w_3|w_{1:2})\cdots P(w_n|w_{1:n-1})$$
if make use of the *markov-assumption*:
$$P(w_{1:n})\approx\prod_{i=1}^n P(w_i|w_{i-k:i-1})$$
where k is a constant integer (k-th order markov assumption)

### Evaluates
 
given a text corpus of n words $w_1, \cdots, w_n$. the *perplexity* of a language model function LM with respect to the corpus is:
$$2^{-\frac{1}{n}\sum_{i=1}^n\log_2 LM(w_i|w_{1:{i-1}})}$$
($LM(w_i|w_{1:i-1})$ is the assigned probability to $w_i$)
good language models have lower perplexity values.

### Traditional Approaches

provide good estimates $\hat{p}(w_{i+1}=m|w_{1:i})\approx\hat{p}(w_{i+1}=m|w_{i-k:i})$. the maximum likelihood estimate is:
$$\hat{p}_{MLE}(w_{i+1}=m|w_{i-k:i})=\frac{\#(w_{i-k:i+1})}{\#(w_{i-k:i})}$$
if the event $w_{i-k:i+1}$ was never observed in the corpus ($\#(w_{i-k:i+1})=0$), the probability would be assigned to 0. To avoid it, we have to use *smoothing techniques*:
$$\hat{p}_{MLE}(w_{i+1}=m|w_{i-k:i})=\frac{\#(w_{i-k:i+1})+\alpha}{\#(w_{i-k:i})+\alpha|V|}$$
where $|V|$ is the size of the vocabulary and $0 < \alpha \leq 1$.
another popular family of approaches is using *back-off*:
$$\hat{p}_{MLE}(w_{i+1}=m|w_{i-k:i})=\lambda_{w_{i-k:i}}\frac{\#(w_{i-k:i+1})}{\#(w_{i-k:i})}\\+(1-\lambda_{w_{i-k:i}})\hat{p}_{MLE}(w_{i+1}=m|w_{i-(k-1):i})$$
if the kgram was not observed, compute an estimate based on a (k-1)gram.

limitations:

 - designed by hand, hard to add more creative conditioning contexts
 - hard to scale toward larger ngrams
 - lack of generalization

### Neural Language Models

more flexible, better generalization.
limitations:

 - much more expensive to predict
 - costly to work with large vocabulary sizes and train corpora

## Pre-trained Word Representations

### Approaches

 - unsupervised: cluster, similar words have similar vectors
 - byproduct of language modeling: create word vectors during training
 - supervised: predict word from its context or predict context from the word
  
### Using Pre-trained Embeddings

 - normalize to unit length? loss information
 - fine-tuning based on the task? only train some words would hurt their relations with other words in the pre-trained matrix
 - if the dataset is big enough, train your customed word vectors from the dataset may be a better solution

### Count-based Methods (Distributional Approach)

**distributional hypothesis**: words that occur in the same contexts tend to have similar meanings.

#### Word-context Matrices

denote by $V_W$ the set of words, by $V_C$ the set of possible contexts. the matrix $\mathbf{M}^f\in \mathbb{R}^{|V_W|\times|V_C|}$ is the word-context matrix, defined as $\mathbf{M}^f_{i,j}=f(w_i, c_j)$, where f is an association measure of the strength between a word and a context, and $w_i$ is the i-th word and $c_j$ is the j-th context.

distance between vectors can be measured with *cosine similarity*, or *generalized Jacaard similarity*:
$$\text{sim}_{Jacaard}(\mathbf{u},\mathbf{v})=\frac{\sum_i\min(\mathbf{u}_i, \mathbf{v}_i)}{\sum_i\max(\mathbf{u}_i, \mathbf{v}_i)}$$

$f(w_i,c_j)$ is often defined as $\#(w_i,c_j)$ or $\frac{\#(w_i,c_j)}{|D|}$. However, very common contexts (eg. a cat, the cat) would receive high scores. Therefore, we use the metric *pointwise mutual information(PMI)*:
$$PMI(x,y)=\log\frac{P(x,y)}{P(x)P(y)}\\
f(w,c)=PMI(w,c)=\log\frac{\#(w,c)\cdot|D|}{\#(w)\cdot\#(c)}$$
to avoid that $(w,c)$ was never observed in the corpus, we use *positive PMI(PPMI)*:
$$PPMI(w,c)=\max(PMI(w,c),0)$$
However, PMI and PPMI tend to assign high value to rare events that occur together.

#### Window based Co-Occurence Matrix

we count the number of times each word appears inside a window of a particular size around the word of interest.
output: a $|V|\times|V|$ matrix.

#### SVD

Considering that the count-based matricies have many columns, the *singular value decomposition(SVD)* could be used to reduce dimension.
$$X=USV^T$$
select the first k columns of U to get a k-dimensional word vectors.
https://www.zhihu.com/question/22237507/answer/53804902

### Distributed Representations Methods (Distributed Approach)

also vector representation, but a given aspect of meaning may be captured by a combination of many dimensions, and that a given dimension may contribute to capturing several aspects of meaning.

#### Word2Vec

based on distributional hypothesis. use two algorithms: *skip-grams(SG)* and *continuous-bag-of-words(CBOW)*, and two training methods: *hierarchical softmax* and *negative sampling*.

**Skip-Grams Algorithm**
for each word, select it as the center word, and 2c words surrounding it as context words. each word gets two vector representations: center word representation (when it is a center word) and context word representation (when it is a context word) (for math convenience and model performance). the goal is to maximize:
$$\frac{1}{T}\prod_{t=1}^T\prod_{-c\leq j\leq c,j\ne0}p(w_{t+j}|w_t, \theta)\\
p(w_o|w_c)=\frac{\exp(u_o^Tv_c)}{\sum_{w=1}^V\exp(u_w^Tv_c)}$$
where V is the size of the vocabulary.
(p.s. we choose softmax but not average $\frac{u_o^Tv_c}{\sum_wu_w^Tv_c}$ is to enlarge the bigger numbers and to avoid the probability histogram to be 'flat')
or to minimize:
$$J(\theta)=-\frac{1}{T}\sum_{t=1}^T\sum_{-c\leq j\leq c,j\ne0}\log p(w_{t+j}|w_t,\theta)\\=-\frac{1}{T}\sum_{t=1}^T\sum_{-c\leq j\leq c,j\ne0}u_o^Tv_c-\log(\sum_{w=1}^V\exp(u_w^Tv_c))$$
we have $2dV$ trainable variables, where d is the dimension of representation vectors (center word vectors + context word vectors). for each trainable vector, we use stochastic gradient descent to update it (update 1 center word vector and 2c context word vectors at each iteration).
After finishing training, the center word representions are used as the word vectors and the context representations are dropped.

**continuous bag-of-words**

the skip-gram model assumes that context words are generated based on the central target word. the CBOW model assumes that the central target word is generated based on the context words.

$$p(w_c|w_{o_1},\cdots,w_{o_{2m}})\\=\frac{\exp(\frac{1}{2m}u_c^T(v_{o_1}+\cdots+v_{o_{2m}}))}{\sum_{w=1}^V\exp(\frac{1}{2m}u_w^T(v_{o_1}+\cdots+v_{o_{2m}}))}$$
$w_{o_1},\cdots,w_{o_{2m}}$ are 2m context words (m is the window size). the training of CBOW is about the same as skip-gram.

**negative sampling**

at each SGD iteration, we have to compute V times of inner products, which is very unefficient. for center word - context word pair, we select 5 to 20 random words as negative samples, and use this loss function (softmax -> sigmoid, reduce lots of computation):
$$\log p(w_{t+j}|w_t,\theta)=log\sigma(u_{t+j}^Tv_t)+\sum_{k\sim P(w)}[\log(1-\sigma(u_k^Tv_t))]\\=log\sigma(u_{t+j}^Tv_t)+\sum_{k\sim P(w)}[\log\sigma(-u_k^Tv_t)]$$
where $P(w)$ is the sampling distribution. $P(w)$ is set to be $f(w)^{3/4}$, where $f(w)$ is the frequency of word w in the corpus (being raised to the $3/4$ power is to increase the probability of rare words).

**hierarchical softmax**

hierarchical softmax is another training method like negative sampling, in order to reduce the computational overhead while normalizing the p.
we first construct a binary tree whose leaf nodes represent every word in the vocabulary. let:
$$p(w_o|w_c)\\=\prod_{j=1}^{L(w_o)-1}\sigma(\langle n(w_o,j+1)=\text{leftChild}(n(w_o,j))\rangle\cdot u_{n(w_o,j)}^Tv_c)$$
$L(w_o)$ is the length of path from the root to the leaf node of word w. $n(w,j)$ is the j-th node on this path, with the context word vector $u_{n(w,j)}$. (hierarchical softmax architecture only assigns each word one vector representation, and one vector representation for each inner node) $\langle x\rangle$ is equal to 1 if x is true, else -1.
by this definition, we have
$$\sum_{w\in V}P(w|w_c)=1$$
which means we do not need further softmax, and therefore reduce the computional complexity from $O(V)$ to $O(\log(V))$.

**subsampling of frequent words**

the very frequent words (e.g. 'a', 'the') provide less information value than the rare words. also, the vector representations of frequent words do not change significantly after training on several million examples. therefore, we need to counter the imbalance between the rare and frequent words.
each word $w_i$ i the training set is discarded with probability:
$$P(w_i)=1-\sqrt{\frac{t}{f(w_i)}}$$
where $f(w_i)$ is the frequency of word $w_i$ and $t$ is a chosen threshold (typically around $10^{-5}$.

#### fastText

treat each word as a sequence of characters to extract the n-grams, e.g. 3-grams: 'where' -> '< wh', 'whe', 'her', 'ere', 're>' (add '<' and '>' to the begin and end to distinguish the subwords used as prefixes and suffixes).
each subword g has a vector $z_g$, and the vector of the whole word w is $u_w=\sum_{g}z_g$.
advantages:

 - can obtain better vectors for more uncommon complex words, even not existing in the dictionary
 - introduce morphological information to word2vec

#### GloVe

let $x_{ij}$ be the count of word $w_j$ in all the context windows for center word $w_i$ in the whole dataset. the loss function of GloVe is:
$$J(\theta)=\sum_{i,j=1}^Wh(x_{ij})(u_j^Tv_i+b_i+c_j-\log x_{ij})^2$$
where $b_i,c_j$ are bias items for center and context words. the function $h(x)=\begin{cases}(x/c)^\alpha,&x<c\\1,&x\ge c\end{cases}$. ($c,\alpha$ are constants)
intuitively, GloVe reduces the loss between the similarity of two vectors $u_j^Tv_i$, and their co-occurence probability $x_{ij}$. $h(x_{ij})\in[0,1]$ is the weight of loss between $w_i,w_j$.

notice that if $w_j$ appears in the context window of $w_i$, $w_i$ would also appear in the context window of $w_j$. so in GloVe context words are equivalent to center words. after learning vectors, GloVe sums up the two vectors as the final word vectors.

### the Choice of Contexts

#### Window Approach

looking at a sequence of 2m+1 words, the middle word is called the *focus word* and the m words to each side are the *contexts*.

parameters:

 - window size: larger windows tend to produce mor topical similarities, while smaller windows tend to produce more functional and syntactic similarities.
 - positional windows: indicating for each context word also its relative position to the focus words (e.g, 'the'->'the:+2' indicating 'the' appears two positions to the right of the focus word)
 - subsample, skipping, normalization, filter, variable window size, ...

#### Sentences, Paragraphs, Documents

use all words in sentences, paragraghs or documents to be the contexts (just like a very large window).
capture topical similarity.

#### Syntactic Window

the text is automatically parsed using a dependency parser, and the context of a word is taken to be the words that are in its proximity in the parse tree.
produce highly functional similarities.

#### Multilingual

using multilingual, translation-based contexts.

#### Character-Based and Sub-Word Representations

derive vector representation of a word from the characters that compose it.
helpful to represent those words that are not in the vocabulary. Could be combined with word-based representations.
Words can also be decomposed to 'meaningful units', which act as extra features.

### Dealing with Multi-Word Units and Word Inflections

deterministically pre-processing the text such that it better fits the desired definitions of words.

### Limitations of Distributional Methods

 - definition of similarity: this definition in distributional approaches is completely operational: words are similar if used in similar contexts. But in practice, there are many facets of similarity. different metrics cause different kinds of similarity.
 - Black Sheeps: people are less likely to mention known information thatn they are to mention novel one. which causes a well-documented bias in people's use of language.
 - antonyms: models tend to judge antonyms as very similar to each other since they ofen appear in similar contexts.
 - corpus biases: the distributional methods reflect the usage patterns in the corpora on which they are based, and the corpora in turn reflect human biases in the real world.
 - Lack of Context: context independent. A word often has several meanings, and it is not appropriate to represent all of its meanings with a single vector.

### *Vectors of Sentences

CS224n Lesson 2

#### Weighted Bag-of-Words + Remove Some Special Direction

 - for all sentence s in S do
$$v_s\leftarrow\frac{1}{|s|}\sum_{w\in s}\frac{a}{a+p(w)}v_w$$
where $p(w)$ is the frequency of word w, and a is a constant.
 - compute the first principal component u of $\{v_s:s\in S\}$, then for all sentence s in S do
$$v_s\leftarrow v_s-uu^Tv_s$$

### Using Pre-Trained Word Embeddings

introduce some applications of pre-trained word embeddings.

#### Word Similarity

$$\text{sim}(w_i,w_j)=\frac{w_i\cdot w_j}{||w_i||_2||w_j||_2}$$

#### Odd-One Out

given a list of words, finding out the one that does not belong.
compute the similarity between each word to the average word vector, and return the least similar word.

#### Short Document Similarity

consider two documents $D_1=w_1^1,\cdots,w_m^1$,$D_2=w_1^2,\cdots,w_n^2$,
$$\text{sim}_{\text{doc}}(D_1,D_2)=\sum_{ij}\text{sim}(w_i^1,w_j^2)$$
if word vectors are all normalized,
$$\text{sim}_{\text{doc}}(D_1,D_2)=(\sum_iw_i^1)\cdot(\sum_jw_j^2)$$

#### Word Analogies

e.g. man:woman -> king:?

$$\text{analogy}(m:w\rightarrow k:?)\\
=\text{argmax}_{v\in V/\{m,w,k\}}cos(v,k-m+w)\\
=\text{argmax}_{v\in V/\{m,w,k\}}cos(v,k)-cos(v,m)+cos(v,w)\\
=\text{argmax}_{v\in V/\{m,w,k\}}\frac{cos(v,k)cos(v,w)}{cos(v,m)+\epsilon}$$

#### Retrofitting

if one has access to a representative and relatively large list of word pairs (a pair means the two words are related) that reflects the desired similarity better than the word embeddings (maybe labelled by humans), the *retrofitting* method can improve the quality of the word embedding vectors using such a list.
let the embedding matrix be E, and the list of pairs be g. the goal of retrofitting is to find such a matrix:
$$\text{argmin}_{\hat{E}}\sum_i(\alpha_i||\hat{E}_{[w_i]}-E_{[w_i]}||^2+\sum_{(w_i,w_j)\in g}\beta_{ij}||\hat{E}_{[w_i]}-\hat{E}_{[w_j]}||^2)$$
in practice, $\alpha_i$ are typically set uniformly to 1, and $\beta_{ij}$ is set to the inverse of num of related words of $w_i$.

#### Projections

given two embedding matrices $E_L$ and $E_S$. they are seperately trained, and the size of $E_S$ is smaller than that of $E_L$. perhaps the quality of $E_S$ is better than $E_L$, and there is some overlap between these two matrices. we can learn a projection matrix $M$,
$$\text{argmin}_M\sum_{w\in V_S\cap V_L}||E_{[w]}^L\cdot M-E_{[w]}^S||$$
to map rows of $E_L$ to the vector space of $E_S$ (maybe to improve the quality of $E_L$).

---
# Specialized Architectures

---

## Sentence Meaning Inference: MLP
in the *natural language inference task*, or *recognizing textual entailment*, given two texts $s1, s2$, you need to decide if $s_1$ entails $s_2$, or if $s_1$ contradicts $s_2$, or if they are neutral.

the model in this paper,
> Ankur P. Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model for natural language inference. In Proc. of EMNLP, 2016. DOI: 10.18653/v1/d16-1244.

accepts $s_1$ and $s_2$ as inputs at the same time. for each word $w_i^{s_1}$ in sentence $s_1$, we compute a $l_{s_2}$-dimensional (length of $s_2$) vector of its similarities to words in sentence $s_2$:
$$\alpha_i^{s_1}=\text{softmax}(\text{sim}(w_i^{s_1},w_1^{s_2}),\cdots,\text{sim}(w_i^{s_1},w_{{l_{s_2}}}^{s_2}))$$
where
$$\text{sim}(w_1,w_2)=\text{MLP}^{transform}(w_1)\cdot\text{MLP}^{transform}(w_2)$$
then similarly compute for words in $s_2$:
$$\alpha_i^{s_2}=\text{softmax}(\text{sim}(w_1^{s_1},w_i^{s_2}),\cdots,\text{sim}(w_{l_{s_1}}^{s_1},w_{i}^{s_2}))$$
then
$$\bar{w}_i^{s_2}=\sum_{j=1}^{l_{s_2}}\alpha_{i\ [j]}^{s_1}w_j^{s_2}\\
\bar{w}_j^{s_1}=\sum_{i=1}^{l_{s_1}}\alpha_{i\ [j]}^{s_2}w_i^{s_1}$$
a vector $\bar{w}_i^{s_2}$ captures the weighted mixture of words in sentence $s_2$ that are triggered by the i-th word in $s_1$.

aggregate data from word pairs:
$$v_i^{s_1}=\text{MLP}^{pair}([w_i^{s_1};\bar{w}_i^{s_2}])\\
v_j^{s_2}=\text{MLP}^{pair}([w_j^{s_2};\bar{w}_j^{s_1}])\\
v^{s_1}=\sum_iv_i^{s_1}\\
v^{s_2}=\sum_jv_j^{s_2}\\
\hat{y}=\text{MLP}^{decide}([v^{s_1};v^{s_2}])$$

## Ngram Detectors: CNN

one-dimensional convolution.
you can image the input of the convolutional layer as a 'picture', whose shape is $1\times n\times d$, where n is the length of the sentence and d is the dimension of word vectors. l filters (size is k) moves from left to right, and results in a $1\times n\times l$ shape output matrix (if padding). the weight matrix would be $k\times d\times l$.
though CNN can capture some local patterns, it disregards the order of patterns that are far apart in the sequence (that's why we need RNN).

## Modeling Sequences and Stacks: RNN

state: $s_i=R(x_i, s_{i-1})$
output: $y_i=O(s_i)$

![image.png-22.3kB][1]

### Acceptor

only observe the final state, and then decide on an outcome.
e.g. sentiment recognition...

### Encoder

the final output is treated as an encoding of the information in the sequence, and is used as additional information together with other signals.
e.g. document summarization...

### Transducer

produce an output $\hat{t}_i$ for each input. compute a local loss signal $L_{local}(\hat{t}_i,t_i)$ for each the output $\hat{t}_i$ based on a true label $t_i$. the loss for the unrolled sequence will then be $\sum_iL_{local}(\hat{t}_i,t_i)$.
e.g. sequence tagger...

## Bidirectional RNNS (BIRNN)

maintains two seperate states $s_i^f$ and $s_i^b$ for each input position i. the forward state $s_i^f$ is based on $x_1,\cdots,x_i$, while the backward state $s_i^b$ is based on $s_n, \cdots,x_i$. the output at position i is based on the concatenation of the two output vectors.

---

# RNN

---
## CBOW as RNN
$$s_i=R(x_i,s_{i-1})=s_{i-1}+x_i\\
y_i=O(s_i)=s_i$$
simulate the CBOW model. the sequential nature of the data is ignored.

## Elman Network (Simple-RNN)

$$s_i=R(x_i,s_{i-1})=g(s_{i-1}W^s+x_iW^x+b)\\
y_i=O(s_i)=s_i$$
where W is weight matrix, g is the activation function.
adding the activation function is very crucial, as it makes the network sensitive to the order of the inputs.
hard to train effectively because of the vanishing gradients problem (gradients in later steps diminish quickly in the back-prop progress, and do not reach earlier input signals -> hard to capture long-range dependencies).

## Gated Architectures

can control the memory access.

### LSTM Long Short-Term Memory

explicitly designed to solve the long-term dependency problem. 

![image.png-94.5kB][2]

three gates: i - input, f - forget, o - output

$$f_t=\sigma(W_f\cdot[h_{t-1},x_t]+b_f)\\
i_t=\sigma(W_i\cdot[h_{t-1},x_t]+b_i)\\
o_t=\sigma(W_o\cdot[h_{t-1},x_t]+b_o)$$

the state at time t is composed of two vectors: $c_t, h_t$, where $c_t$ is the memory component and $h_t$ is the hidden state component.
$\tilde{C}_t$ is the candidate vector, which could be added to the state:
$$\tilde{C}_t=\text{tanh}(W_C\cdot[h_{t-1},x_t]+b_C)$$
gate f controls how much of the previous memory to keep, and gate i controls how much of the proposed update to keep:
$$C_t=f_t\odot C_{t-1}+i_t\odot\tilde{C}_t$$
then, output $h_t$:
$$h_t=o_t\odot \text{tanh}(C_t)$$
the gating machanisms allow for gradients related to the memory part $C_t$ to stay high across very long time ranges, because it gets rid of the repeated multiplication of a single matrix.
($C_{t-1}, C_t$ are related linearly but not by multiplication, so the gradient can 'flow' across the sequence in long term).

### GRU Gated Recurrent Unit

use fewer gates (faster to train), but also effective (however, sometimes worse than LSTM).

![image.png-72.3kB][3]

combines the input and forget gate to a single 'update' gate $z_t$. $h_t$ and $C_t$ are also merged.
the gate $r_t$ controls the access to the previous state $h_{t-1}$ and computes a proposed update $\tilde{h}_t$.

## Dropout in RNN

![image.png-112kB][4]

*Variation RNN*: the dropout masks are sampled once per sequence, and not once per time step. (colored connnections represent dropped-out inputs, with different colors corresponding to different dropout masks).

## Modeling with Recurrent Networks: Acceptors

read in an input sequence, output binary or multi-class answer at the end.

### Sentence-Level Sentiment Classification

question description: given a sentence, we need to assign it one of two values: POSITIVE or NEGATIVE.

simplest model:
$$x_{1:n}=E_{[w_1]},\cdots,E_{[w_n]}\\
\hat{y}=\text{softmax}(\text{MLP}(\text{RNN}(x_{1:n})))$$
($E_{[w_i]}$ means embedding vector of word $w_i$)
**extension** 1: BiRNN:
$$x_{1:n}=E_{[w_1]},\cdots,E_{[w_n]}\\
\hat{y}=\text{softmax}(\text{MLP}([\text{RNN}^f(x_{1:n});\text{RNN}^b(x_{n:1})]))$$
**extension** 2: hierarchical architecture (split the sentence into smaller spans based on punctuation)
$$x_{1:l_i}^i=E_{[w_1^i]},\cdots,E_{[w_{l_i}^i]}\\
z_i=[\text{RNN}^f(x_{1:l_1}^i);\text{RNN}^b(x_{l_1:1}^i)]\\
\hat{y}=\text{softmax}(\text{MLP}(\text{RNN}(z_{1:m})))$$
($w_{1:l_i}^i$ are words of the i-th span)

### Document-Level Sentiment Classification

hierarchical architecture:
each sentence $s_i$ is encoded using a gated RNN producing a vector $z_i$, and the vectors $z_{1:n}$ are then fed into a second gated RNN, producing a vector $h$ which is then used for prediction $\hat{y}=\text{softmax}(\text{MLP}(h))$.
**variant**: $h_i=\text{RNN}(z_i)$, $\hat{y}=\text{softmax}(\text{MLP}(\frac{1}{n}\sum_ih_i))$.

## Modeling with RNN: Feature Extractors

### Extract Character-Level Features from Words

Characters in a word contain lots of information (e.g. prefix, suffix, capitalization, hyphens, digits...). we can use RNN to extract character-level features from the word to enhance the word embedding vector.
$$x_i=[E_{[w_i]};\text{RNN}^f(c_{1:l});\text{RNN}^b(c_{l:1})]$$
($c_i$ is the vector representation of the i-th character)
the forward-running RNN focuses on capturing suffixes, the backward-running RNN focuses on prefixes.
**variant**: use convolution + pooling layers to shorten the embeddings.

## Generator

predicting process:

![image.png-106.1kB][5]

train it as a transducer. the input sequence is $\text{<s>}, w_1, w_2,\cdots,w_n$. the output sequence is $w_1,w_2,\cdots,w_n,\text{</s>}$.

we can add condition information to it:

![image.png-138.6kB][6]

where $c$ is the context vector.

if $c$ is the output of another RNN (encoder), this model is called *encoder-decoder*. it first compute encoder outputs to initialize the decoder state, and then returns the decoder outputs.

the *Sequence to Sequence* (seq2seq) model is based on the encoder-decoder architecture to generate a sequence output for a sequence input. the hidden state of the encoder is used directly to initialize the decoder hidden state to pass information from the encoder to the decoder.

![image.png-70kB][7]

## Unsupervised Sentence Similarity

to train sentence encoder such that similar sentences are encoded to similar vectors.

### Auto Encoding
the auto-encoding approach is a conditioned generation model in which a sentence is encoded using a RNN, and the decoder attempts to reconstruct the input sentence. the decoder RNN is thrown away, and the encoder is used to generate sentence representations.

### Machine Translation

use the encoder part of a seq2seq translation model. because intuitively the vectors produced by the encoder for generating translation extract the essence of the sentence.

### Skip-thoughts

the author thought that the sentences are similar if they appear in similar contexts.
also a conditioned generation model, where an RNN encoder maps a sentence to a vector, and then one decoder to reconstruct the **previous** sentence based on the encoded representation, and a **second** decoder is trained to reconstruct the following sentence.

### Syntactic Similarity

also a encoder-decoder, but decoder the encoded vector to a linearized parse tree as a stream of bracketing decisions, i.e., from
> the boy opened the door

to
> (S (NP DT NN ) (VP VBD (NP DT NN ) ) )

## Conditioned Generation with Attention

the encoder-decoder with attention architecture encodes a length n input sequence $x_{1:n}$ using a BiRNN, producing n vectors $c_{1:n}$ (keep outputs of every time step), then the decoder use these vectors as a read-only memory representing the conditioning sentence: at any stage j of the decoding process, it chooses which of the vectors $c_{1:n}$ it should attach to:
$$c^j=\text{attend}(c_{1:n},\hat{t}_{1:j})\\
s_{j+1}=R(s_j,[\hat{t}_j;c^j])\\
p(t_{j+1}=k|\hat{t}_{1:j},x_{1:n})=f(O(s_{j+1}))$$
where the `attend` function is a weighted average of $c_{1:n}$:
$$c^j=\sum_{i=1}^n\alpha_{[i]}^j\cdot c_i$$
where $alpha^j$ is the vector of attention weights for stage j:
$$\bar{\alpha}^j_{[i]}=\text{MLP}^{\text{att}}([s_j;c_i])\\
\alpha^j=\text{softmax}(\bar{\alpha}_{[1]}^j,\cdots,\bar{\alpha}_{[n]}^j)$$

![image.png-138.3kB][8]

(the $BI_E$ means BiRNN encoder; $E_[the]$ means the embedding vector of the word `the`)

more interpretable than normal encoder-decoder. during the decoding process, one can look at the produced attention weights $\alpha^j$ and see which areas of the source sequence the decoder found relevant when producing the given output.


  [1]: http://static.zybuluo.com/twoer2/xm6id09oi4udjlj9l5548j0y/image.png
  [2]: http://static.zybuluo.com/twoer2/uvrncrf3je4senk9j7y5vgvd/image.png
  [3]: http://static.zybuluo.com/twoer2/ugk0qglu4murarbb0aiekso3/image.png
  [4]: http://static.zybuluo.com/twoer2/qupnzsy5w77dy93lh7xvtgep/image.png
  [5]: http://static.zybuluo.com/twoer2/5fnkemdxjors162ymxijduag/image.png
  [6]: http://static.zybuluo.com/twoer2/ijga8ersmottqfzojn3ifleo/image.png
  [7]: http://static.zybuluo.com/twoer2/x0htn14krs1rkfetmn7ry138/image.png
  [8]: http://static.zybuluo.com/twoer2/zpobhzyad145h7igm8ka1902/image.png

