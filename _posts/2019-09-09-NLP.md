---
layout: post
title: from raw text to NLP models
excerpt: "A brief short summary of training NLP models."
categories: [CS]
comments: true
image:
  feature: NLP_cover.jpg
---

## Reference

 - Neural Network Methods for Natural Language Processing, Yoav Goldberg.
 - [CS224n](https://web.stanford.edu/class/cs224n/ )
 - [Dive into Deep Learning](<https://www.d2l.ai/>)
 - Hope you can give me some advice at the comment area if I make some mistakes.



---

## A Common Pipeline

From a natural language dataset to a well-trained useful model, we may go through the following steps:

- *textual data*:  raw text

  Before you begin to train your model, you have to prepare some raw texts, or *corpus*, which may be crawled from Internet, or may be part of a large database, etc. Most machine learning researchers would agree that the larger corpus, the better model performance. At the same time, the quality of the corpus is also vital. For example, Wikipedia is a good source of high-quality texts, because it has less grammatical errors, and its language is more precise. As well, the corpus should be suit for your task. It would be a waste of time to train a news-classification model with a corpus of C++ codes. 

- *feature extraction* or *feature representation*: mapping from textual data to real valued vectors

  Extracting features from the raw texts is important to do further analyzing and computing. When determining the similarity between two texts, we can regard the number of different words in each text as a feature, and make a conclusion by computing the difference between their feature values. As well, we can think the source of texts as a feature to do the computing. By focusing on different properties of the data, we can represent the data in different ways. Lots of efforts have been made to represent articles, words, sentences, etc., as real valued vectors.

  One of the key problems in NLP is to represent words as vectors. Because words are components of sentences and documents, we can combine word vectors to represent sentences and documents. Lots of approaches have been proposed, from manually extracting language features, to generating automatically with the help of neural networks, ... Some of them are classic and intuitive, like *TF-IDF*, *Word-context Matrices*, while some of them have achieved very good results, like *word2vec*, *GloVe*. A good vector representation can contain most of the information that a word can convey. To achieve this goal, NLP researchers have to overcome some difficult challenges which are results of the ambiguity of natural languages. For example, a word may have several totally different meanings, and its exact meaning can only be inferred from the context.

- *inputs*: combine single vectors into input vectors

- *training*: feed the input into a neural network

## Feature Extraction

By parsing and structuring natural language, linguists have manually designed some linguistic properties to analyze texts. One of the example is *Bag of Words*. Suppose we believe that documents which contain similar words are similar to each other in meanings. We can represent a document as a vector by counting how many different words it contains. If two documents contain similar words, the distance between their vector representations would be short.

*Bag of Words* and its extension *Continuous Bag of Words*, and other classic linguistic properties, like *TF-IDF*, etc., may be known to all NLP enthusiasts and students. These features really promoted the development of NLP. But when it comes to the era of deep learning, we may ask: do we need these man-made features any more? Since a deep neural network can learn representations, and even provide non-linearity, freeing us from designing and combining features. We can just feed the data to this magic black box, and things would be done automatically. With the development of deep learning, I believe the importance of manually designing features before training would be lower and lower. After all, freeing humans from this kind of 'human-learning', and building models of the true 'machine-learning', would push us closer to the essential of artificial intelligence. However, currently these man-made features are still useful. They can help us train our models better in the case that the dataset is not big enough, and guide us to design networks more reasonably.

Much of the earlier NLP works focus on the context information of natural language and do not treat words as atomic symbols. However, when we are going to train a neural network like RNN, most of us would need the help of word representation vectors. A sentence, or a article, is represented as a sequence of these word vectors. Good word vectors have some good properties, which will improve the performance of models significantly.

Word vectors can be sparse, or be dense. and most sparse word vectors are generated by count-based methods, while dense word vectors are generated by iteration-based methods. The simplest word vector is *one-hot vector*, which represents each word as  a \\(\mathbb{R}^{|V|\times 1}\\) vector with all 0s and exactly one 1 at the index of that word in the sorted English language. In this notation, \\(|V|\\) is the size of the vocabulary. For example, the first word in the vocabulary, aardvark, would be represented as:

\\(w^{\text{aardvark}}\begin{bmatrix}1&0&\cdots&0\end{bmatrix}^T\\)

Although simple, one-hot vector could not be used to capture similarity of words directly, because any two one-hot vectors are orthogonal, which means their inner product is always 0.































