---
layout: post
title: Neural Machine Translation in Seq2eq
excerpt: "A brief introduction to seq2seq, with the example of NMT."
categories: [NLP]
comments: true
---

---
This is a learning note of studying [CS224N](https://web.stanford.edu/class/cs224n/ ). Seq2seq is a beautiful recurrent neural network (RNN) variant, whose inputs and outputs both are sequences. Its applications include machine translation, speech recognition, text summarization, conversational models, etc. Seq2seq is a large field to explore, and research topics related to it are still active. This post only discusses the very basic part of seq2seq, and introduces its application on machine translation, which was proposed together with the model in its original paper.

An simple implementation of seq2seq could be found in my [Github Repository]().



## Seq2seq

The encoder-decoder structure was firstly proposed by Cho (2014). This model consists of two RNNs, in which one is the encoder and the other is the decoder. The encoder receives a variable-length sequence input, *encoding* it to be a vector $c$, which could be regarded as a summary of the sequence. Then, the decoder would predict the target sequence words conditioned on this summary $c$ (you can think it as the context) and previously predicted sequence words.

![https://two2er.github.io/img/seq2seq/encoder_decoder.jpg]()

Note that during training, at the $t$-th time step, the decoder would predict the target sequence words conditioned on $c$ and the $t$-th word in the gold-standard target sentence rather than the last prediction. The loss function could be softmax cross-entropy loss, and the encoder and decoder are trained together.

The seq2seq is based on the encoder-decoder structure. In a seq2seq model, the last hidden state is used to initialize the decoder hidden state, which acts as the context. In the original paper, some tricks improving the performance of the model were introduced. They are:

1. Reverse the order of words in the input sentence to introduce more short term dependencies. Intuitively, doing this would make the last word of the input sequence "close" to the begin of the target sentence, and easier for the decoder to "get started" on the output.
2. Use multi-layer LSTM, which significantly outperforms single layer LSTM or simple RNN. The LSTM is generally better than simple RNN on capturing long term dependencies, and is easier to train.

Besides these tricks, as described in the CS224N course notes, the seq2seq could be improved by adding other features. One of the most effective ideas is the *Attention* mechanisms. The last hidden state of the encoder might miss sequential information since it compresses the sequence. As the length of the input sequence increases, the last hidden state would be less and less capable to summarize the sequence. To deal with this issue, the Attention is proven to be a clever way. In a decoder augmented by attention, the context vector is no longer the last hidden state of the encoder, but a weighted sum of all hidden states produced by the encoder. At each time step, the weight of the $i$-th hidden state of the encoder is determined by itself, and the current hidden state of the decoder. Since the weights for every hidden state are different, the context seems to pay "attention" on some words, which might be more related to the current word in the target sequence. This is an idea to "align" the input and the target sequences. More details would be shown below.

Another great idea is the *Beam Search*





## Model Details

This part would describe some implementation details of the seq2seq model following the CS224N course note. This implementation not necessarily performs better than others. It is just an example to help understand the seq2seq model. Any hyperparameters and networks could be changed since this is not the only standard.



## Conclusion



## Reference

- *Sequence to Sequence Learning with Neural Networks*. Ilya Sutskever, Oriol Vinyals, Quoc V. Le.
- *Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation*. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio.
- *Neural Network Methods for Natural Language Processing*. Yoav Goldberg.
- [CS224N](https://web.stanford.edu/class/cs224n/ )
- [Dive into Deep Learning](<https://www.d2l.ai/>)